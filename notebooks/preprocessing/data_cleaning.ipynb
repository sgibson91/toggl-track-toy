{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c33ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting filepaths\n",
    "#\n",
    "# Get Current Working Directory\n",
    "CWD = os.getcwd()\n",
    "\n",
    "# Set Relative and Absolute filepaths for raw data\n",
    "RAW_DATA_RELPATH = os.path.join(os.pardir, os.pardir, \"data\", \"raw\")\n",
    "RAW_DATA_ABSPATH = os.path.join(CWD, RAW_DATA_RELPATH)\n",
    "\n",
    "# Set Relative and Absolute filepaths for clean data\n",
    "CLEAN_DATA_RELPATH = os.path.join(os.pardir, os.pardir, \"data\", \"clean\")\n",
    "CLEAN_DATA_ABSPATH = os.path.join(CWD, CLEAN_DATA_RELPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find data files\n",
    "datafiles = fnmatch.filter(os.listdir(RAW_DATA_ABSPATH), \"*time_entries*.csv\")\n",
    "datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data files into single dataframe\n",
    "filename = os.path.join(RAW_DATA_ABSPATH, datafiles[0])\n",
    "df = pd.read_csv(\n",
    "    filename,\n",
    "    converters={\"Duration\": pd.to_timedelta},\n",
    "    parse_dates={\n",
    "        \"dt_start\": [\"Start date\", \"Start time\"],\n",
    "        \"dt_end\": [\"End date\", \"End time\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "for datafile in datafiles[1:]:\n",
    "    filename = os.path.join(RAW_DATA_ABSPATH, datafile)\n",
    "    tmpdf = pd.read_csv(\n",
    "        filename,\n",
    "        converters={\"Duration\": pd.to_timedelta},\n",
    "        parse_dates={\n",
    "            \"dt_start\": [\"Start date\", \"Start time\"],\n",
    "            \"dt_end\": [\"End date\", \"End time\"],\n",
    "        },\n",
    "    )\n",
    "    df = pd.concat([df, tmpdf])\n",
    "\n",
    "# Drop unwanted columns\n",
    "df.drop(\n",
    "    [\"User\", \"Email\", \"Client\", \"Task\", \"Billable\", \"Tags\", \"Amount ()\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Add `day_of_week` and `week_number` columns\n",
    "df[\"day_of_week\"] = df[\"dt_start\"].dt.day_name()\n",
    "df[\"week_number\"] = df[\"dt_start\"].dt.isocalendar().week\n",
    "\n",
    "# Drop rows relating to weekends\n",
    "df[~df.day_of_week.str.contains(\"Saturday\") & ~df.day_of_week.str.contains(\"Sunday\")]\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Sort by `dt_start` column\n",
    "df.sort_values(\"dt_start\", axis=0, inplace=True, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anonymising\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Anonymise the `Project` column\n",
    "projects = Counter(df[\"Project\"].values)\n",
    "project_converters = {\n",
    "    project: f\"Project {string.ascii_uppercase[i]}\"\n",
    "    for i, project in enumerate(projects.keys())\n",
    "}\n",
    "\n",
    "# Anonymise the `Description` column\n",
    "task_converters = {\n",
    "    \"\"\"\n",
    "    REDACTED\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Add empty Tasks column\n",
    "df[\"Task\"] = np.zeros(len(df))\n",
    "\n",
    "# Begin replacing columns\n",
    "for i, row in df.iterrows():\n",
    "    # Replace Project\n",
    "    df.loc[i, \"Project\"] = project_converters[row[\"Project\"]]\n",
    "\n",
    "    # Insert Task\n",
    "    for key, values in task_converters.items():\n",
    "        if row[\"Description\"] in values:\n",
    "            df.loc[i, \"Task\"] = key\n",
    "\n",
    "# Drop the `Description` column\n",
    "df.drop(\"Description\", axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_filepath(list_of_files: list, filepath: os.path) -> os.path:\n",
    "    \"\"\"\n",
    "    Construct the filename and path to output a CSV file to. This function\n",
    "    concatenates dates from the filenames of all data files that were used\n",
    "    in constructing the pandas DataFrame to save and creates a date range\n",
    "    that describes all the data for the filename.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "      list_of_files (list): The list of all the datafiles the DataFrame was\n",
    "          constructed from\n",
    "      filepath (os.path): The filepath to the output clean data dir\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      (os.path): The constructed filepath and name for the output CSV file\n",
    "    \"\"\"\n",
    "    found_dates = []\n",
    "    date_pattern = re.compile(r\"[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\")\n",
    "\n",
    "    for datafile in list_of_files:\n",
    "        found_dates.extend(re.findall(date_pattern, datafile))\n",
    "\n",
    "    found_dates.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "    filename = f\"{found_dates[0]}_{found_dates[-1]}.csv\"\n",
    "\n",
    "    return os.path.join(filepath, filename)\n",
    "\n",
    "\n",
    "def save_to_csv(\n",
    "    df_to_save: pd.DataFrame,\n",
    "    list_of_files: list,\n",
    "    filepath: os.path = CLEAN_DATA_ABSPATH,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Function to save a DataFrame to a CSV file\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "      df_to_save (pd.DataFrame): The pandas DataFrame to save as a CSV file\n",
    "      list_of_files (list): List of data filenames that the DataFrame has been\n",
    "          constructed from\n",
    "      filepath (os.path): The filepath to output to CSV file to\n",
    "          Default: CLEAN_DATA_ABSPATH\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      None\n",
    "    \"\"\"\n",
    "    resp = input(\"Would you like to save this file to the `data/clean` dir? [yes/no]: \")\n",
    "\n",
    "    if re.match(\"[Y|y][E|e][S|s]\", resp):\n",
    "        filename = construct_filepath(list_of_files, filepath)\n",
    "        df_to_save.to_csv(filename, index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "save_to_csv(df, datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in timeline data\n",
    "timeline_path = os.path.join(RAW_DATA_ABSPATH, \"timeline.json\")\n",
    "\n",
    "with open(timeline_path) as stream:\n",
    "    timeline = json.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse timeline data into a DataFrame\n",
    "tl_df = pd.DataFrame(columns=[\"Tool\", \"Title\", \"dt_start\", \"dt_end\", \"idle\"])\n",
    "\n",
    "for item in timeline:\n",
    "    tl_df = tl_df.append(\n",
    "        {\n",
    "            \"Tool\": item[\"filename\"],\n",
    "            \"Title\": item[\"title\"],\n",
    "            \"dt_start\": datetime.fromtimestamp(item[\"start_time\"]),\n",
    "            \"dt_end\": datetime.fromtimestamp(item[\"end_time\"]),\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted rows\n",
    "tool_to_be_dropped = [\"Notion\"]\n",
    "title_to_be_dropped = [\"WhatsApp\", \"Telegram Web\", \"YouTube\", \"Twitter\", \"Facebook\"]\n",
    "\n",
    "labels_to_drop = product(tool_to_be_dropped, title_to_be_dropped)\n",
    "for tool, title in labels_to_drop:\n",
    "    tl_df.drop(\n",
    "        tl_df.index[(tl_df[\"Tool\"] == tool) | (tl_df[\"Title\"] == title)].tolist(),\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530acafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify some tool names\n",
    "tool_converters = {\n",
    "    \"GitHub\": [\n",
    "        \"Pull Request\",\n",
    "        \"GitHub\",\n",
    "        \"Issue\",\n",
    "        \"Branches\",\n",
    "        \"Organization profile\",\n",
    "        \"Billing\",\n",
    "        \"alan-turing-institute\",\n",
    "    ],\n",
    "    \"Gitter\": [\"Gitter\"],\n",
    "    \"Azure\": [\"Pipelines\", \"Microsoft Azure\"],\n",
    "    \"Travis\": [\"Travis CI\"],\n",
    "    \"Harvest\": [\"Harvest\"],\n",
    "}\n",
    "\n",
    "prod = product(tl_df.iterrows(), tool_converters.items())\n",
    "for p in prod:\n",
    "    (i, row), (tool, tool_list) = p\n",
    "    truthy_list = list(filter(lambda v: v in row[\"Title\"], tool_list))\n",
    "    if len(truthy_list) > 0:\n",
    "        tl_df.loc[i, \"Tool\"] = tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename instances of `Code` to `VSCode`\n",
    "tl_df.loc[tl_df[\"Tool\"] == \"Code\", \"Tool\"] = \"VSCode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a duration column\n",
    "tl_df[\"Duration\"] = tl_df[\"dt_end\"] - tl_df[\"dt_start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `Title` column, sort by `dt_start` column, and drop NAs\n",
    "tl_df = (\n",
    "    tl_df.drop(\"Title\", axis=1).sort_values(\"dt_start\").dropna().reset_index(drop=True)\n",
    ")\n",
    "tl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2673c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV file\n",
    "out_path = os.path.join(CLEAN_DATA_ABSPATH, \"timeline.csv\")\n",
    "tl_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (toggl-track)",
   "language": "python",
   "name": "toggl-track"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
